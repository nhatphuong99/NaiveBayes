{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar\n",
    "\n",
    "- Sinh viên 1: 1753086 - Tống Lê Thiên Phúc\n",
    "- Sinh viên 2: 1753089 - Nguyễn Lý Nhật Phương"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import TypeVar, List, Tuple\n",
    "\n",
    "X = TypeVar('X')  # generic type to represent a data point\n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "\n",
    "Y = TypeVar('Y')  # generic type to represent output variables\n",
    "\n",
    "\n",
    "def train_test_split(xs: List[X],\n",
    "                     ys: List[Y],\n",
    "                     test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "\n",
    "    # Generate the indices and split them.\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "    \n",
    "    return ([xs[i] for i in train_idxs],  # x_train\n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def precision(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    \n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Really Dumb Spam Filter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Theorem\n",
    "\n",
    "$$P(h|\\mathcal{D}) = \\frac{P(\\mathcal{D}|h)P(h)}{P(\\mathcal{D})}$$\n",
    "\n",
    "- $P(h) = $ **prior probability** of hypothesis $h$\n",
    "- $P(\\mathcal{D}) = $ **prior probability** of training data $\\mathcal{D}$\n",
    "- $P(h|\\mathcal{D}) = $ probability of $h$ given $\\mathcal{D}$ (**posterior probability**)\n",
    "- $P(\\mathcal{D}|h) = $ probability of $\\mathcal{D}$ given $h$ (**likelyhood**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S$: The event \"the message is spam\"\n",
    "\n",
    "$V$: The event \"the message contains the word *viagra*\"\n",
    "\n",
    "\n",
    "- The probability that the message is spam conditional on containing the word *viagra* is:\n",
    "\n",
    "$$P(S|V) = \\frac{P(V|S)P(S)}{P(V|S)P(S) + P(V|\\neg S)P(\\neg S)}$$\n",
    "\n",
    "- Assume that any message is equally likely to be spam or notspam ($P(S) = P(\\neg S) = 0.5$) then:\n",
    "\n",
    "$$P(S|V) = \\frac{P(V|S)}{P(V|S) + P(V|\\neg S)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** If\t50%\tof spam\tmessages\thave\tthe\tword\tviagra,\tbut\tonly 1%\tof\tnonspam messages\tdo,\tthen\tthe\tprobability\tthat\tany\tgiven\tviagra-containing\temail\tis\tspam\tis:\n",
    "$$\\frac{0.5}{0.5+0.01} = 0.98$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Sophisticated Spam Filte\n",
    "\n",
    "- Have a vocabulary of many words $w_1, ... , w_n$\n",
    "- $X_i$: The event \"a message contains the word\t$w_i$\" \n",
    "- The probability that a spam message contains the $i$th word $$P(X_i|S)$$\n",
    "- The probability that a nonspam message contains the $i$th word $$P(X_i|\\neg S)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\tkey\tto\tNaive\tBayes\tis\tmaking\tthe\t(big)\tassumption\tthat\tthe\tpresences\t(or\tabsences)\tof each\tword\tare\tindependent\tof\tone\tanother,\tconditional\ton\ta\tmessage\tbeing\tspam\tor\tnot. Intuitively,\tthis\tassumption\tmeans\tthat\tknowing\twhether\ta\tcertain\tspam\tmessage\tcontains the\tword\t“viagra”\tgives\tyou\tno\tinformation\tabout\twhether\tthat\tsame\tmessage\tcontains\tthe word\t“rolex.”\t\n",
    "\n",
    "$$P(X_1 = x_1, ..., X_n = x_N|S) = P(X_1 = x_1|S)\\times...\\times(X_n = x_n|S)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This\tis\tan\textreme\tassumption.\t(There’s\ta\treason\tthe\ttechnique\thas\t“naive”\tin\tits\tname.) Imagine\tthat\tour\tvocabulary\tconsists\tonly\tof\tthe\twords\t“viagra”\tand\t“rolex,”\tand\tthat\thalf of\tall\tspam\tmessages\tare\tfor\t“cheap\tviagra”\tand\tthat\tthe\tother\thalf\tare\tfor\t“authentic rolex.”\tIn\tthis\tcase,\tthe\tNaive\tBayes\testimate\tthat\ta\tspam\tmessage\tcontains\tboth\t“viagra” and\t“rolex”\tis:\n",
    "\n",
    "$$P(X_1 = 1, X_2 = 1 | S) = P(X_1 = 1|S)P(X_2 = 1|S) = 0.5\\times0.5=0.25$$\n",
    "\n",
    "since\twe’ve\tassumed\taway\tthe\tknowledge\tthat\t“viagra”\tand\t“rolex”\tactually\tnever\toccur together.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\tsame\tBayes’s\tTheorem\treasoning\twe\tused\tfor\tour\t“viagra-only”\tspam\tfilter\ttells\tus that\twe\tcan\tcalculate\tthe\tprobability\ta\tmessage\tis\tspam\tusing\tthe\tequation:\n",
    "\n",
    "$$P(S|X=x)=\\frac{P(X=x|S)}{P(X=x|S)+P(X=x|\\neg S)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\tNaive\tBayes\tassumption\tallows\tus\tto\tcompute\teach\tof\tthe\tprobabilities\ton\tthe\tright simply\tby\tmultiplying\ttogether\tthe\tindividual\tprobability\testimates\tfor\teach\tvocabulary word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In\tpractice,\tyou\tusually\twant\tto\tavoid\tmultiplying\tlots\tof\tprobabilities\ttogether,\tto\tavoid\ta problem\tcalled\tunderflow,\tin\twhich\tcomputers\tdon’t\tdeal\twell\twith\tfloating-point\tnumbers that\tare\ttoo\tclose\tto\tzero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log(ab) = log(a) + log(b)$$\n",
    "$$exp(log(x)) = x$$\n",
    "\n",
    "$$p_1\\times ... \\times p_n \\Leftrightarrow exp(log(p_1) + ... + log(p_n))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine\tthat in\tour\ttraining\tset\tthe\tvocabulary\tword “data”\tonly\toccurs\tin\tnonspam\tmessages\n",
    "\n",
    "$$P(w_i|S) = 0 \t\\Rightarrow P(S|message\\ contains\\ word\\ w_i) = 0$$\n",
    "\n",
    "To\tavoid\tthis\tproblem,\twe\tusually\tuse\tsome\tkind\tof\tsmoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In\tparticular,\twe’ll\tchoose\ta\tpseudocount\t—\tk\t—\tand\testimate\tthe\tprobability\tof\tseeing the\tith\tword\tin\ta\tspam\tas:\n",
    "$$P(X_i|S) = \\frac{k + number\\ of\\ spams\\ containing\\ w_i}{2k + number\\ of\\ spams}$$\n",
    "$$P(X_i|\\neg S) = \\frac{k + number\\ of\\ nonspams\\ containing\\ w_i}{2k + number\\ of\\ nonspams}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Set, List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message: str) -> Set[str]:\n",
    "    message = message.lower() # Convert to lowercase,\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)  # extract the words, and\n",
    "\n",
    "    return set(all_words) # remove duplicates.\n",
    "\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smoothing factor\n",
    "        \n",
    "        self.words: Set[str] = set()\n",
    "        self.word_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.word_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        #count spam and non-spam messages\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "            \n",
    "            # Increment word counts\n",
    "            for word in tokenize(message.text):\n",
    "                self.words.add(word)\n",
    "\n",
    "            if message.is_spam:\n",
    "                self.word_spam_counts[word] += 1 #count times we saw that word in spam messages\n",
    "            else:\n",
    "                self.word_ham_counts[word] += 1 #count times we saw that word in nonspam messages\n",
    "\n",
    "    \n",
    "    def _probabilities(self, word: str) -> Tuple[float, float]:\n",
    "        \"\"\"returns P(word | spam) and P(word | ham)\"\"\"\n",
    "        spam = self.word_spam_counts[word]\n",
    "        ham = self.word_ham_counts[word]\n",
    "        \n",
    "        p_word_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_word_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "        \n",
    "        return p_word_spam, p_word_ham\n",
    "\n",
    "\n",
    "    #use these word probabilities (and our Naive Bayes assumptions) to assign probabilities to message\n",
    "    def predict(self, message: str) -> float:\n",
    "        message_words = tokenize(message)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0 \n",
    "\n",
    "        #iterate through each word in our vocabulary\n",
    "        for word in self.words:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(word)\n",
    "            #if *word* appears in the message,\n",
    "            #add the log probability of seeing it\n",
    "            if word in message_words:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # otherwise add the log probability of _not_ seeing it\n",
    "            # which is log(1 - probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "\n",
    "    \n",
    "    def predict_nonsmooth(self, message: str) -> float:\n",
    "        message_words = tokenize(message)\n",
    "\n",
    "        #iterate through each word in our vocabulary\n",
    "        for word in self.words:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(word)\n",
    "\n",
    "\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Description\n",
    "\n",
    "Split into three parts, as follows:\n",
    "\n",
    "  - spam: 501 spam messages, all received from non-spam-trap sources.\n",
    "\n",
    "  - easy_ham: 2551 non-spam messages.  These are typically quite easy to\n",
    "    differentiate from spam, since they frequently do not contain any spammish\n",
    "    signatures (like HTML etc).\n",
    "\n",
    "  - hard_ham: 250 non-spam messages which are closer in many respects to\n",
    "    typical spam: use of HTML, unusual HTML markup, coloured text,\n",
    "    \"spammish-sounding\" phrases etc.\n",
    "\n",
    "Total count: 3302 messages, with about a 15% spam ratio.\n",
    "\n",
    "After extracting the data we have three\tfolders: spam, easy_ham, and hard_ham. Each folder contains\tmany emails, each contained in a single file. To keep things really simple, we’ll just look at the subject lines of each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the subject line\n",
    "# modify the path to wherever you've put the files\n",
    "path = 'MessagesExample/*/*'\n",
    "data: List[Message] = []\n",
    "    \n",
    "# glob.glob returns every filename that matches the wildcarded path\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename\n",
    "    # There are some garbage characters in the emails, the errors='ignore'\n",
    "    # skips them instead of raising an exception.\n",
    "    with open(filename, errors='ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Message(subject, is_spam))\n",
    "                break  # done with this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "random.seed(0) #to get the same answers as instruction\n",
    "train_data, test_data = split_data(data, 0.75) #train = 0.75, test = 0.25\n",
    "\n",
    "#build classifier\n",
    "classifier = NaiveBayesClassifier() \n",
    "classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 417, (False, True): 282, (True, True): 103, (True, False): 23})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#(actual is_spam, predicted spam probability) \n",
    "predictions = [(message, classifier.predict(message.text))\n",
    "               for message in test_data]\n",
    "\n",
    "# Assume that spam_probability > 0.5 corresponds to spam prediction\n",
    "# and count the combinations of (actual is_spam, predicted is_spam)\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5)\n",
    "                           for message, spam_probability in predictions)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6303030303030303\n",
      "Recall =  0.8174603174603174\n",
      "Precision =  0.2675324675324675\n",
      "F1-score =  0.40313111545988256\n"
     ]
    }
   ],
   "source": [
    "tp = confusion_matrix[(True, True)]\n",
    "tn = confusion_matrix[(False, False)]\n",
    "fp = confusion_matrix[(False, True)]\n",
    "fn = confusion_matrix[(True, False)]\n",
    "\n",
    "acc = accuracy(tp, fp, fn, tn)\n",
    "rec = recall(tp, fp, fn, tn)\n",
    "pre = precision(tp, fp, fn, tn)\n",
    "f1 = f1_score(tp, fp, fn, tn)\n",
    "\n",
    "print(\"Accuracy = \", acc)\n",
    "print(\"Recall = \", rec) #correct positive predicted in total actual positive \n",
    "print(\"Precision = \", pre) #correct positive predicted in total predict positve\n",
    "print(\"F1-score = \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, True): 699, (True, True): 126})\n"
     ]
    }
   ],
   "source": [
    "predictions = [(message, classifier.predict_nonsmooth(message.text))\n",
    "               for message in test_data]\n",
    "\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5)\n",
    "                           for message, spam_probability in predictions)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.15272727272727274\n",
      "Recall =  1.0\n",
      "Precision =  0.15272727272727274\n",
      "F1-score =  0.26498422712933756\n"
     ]
    }
   ],
   "source": [
    "tp = confusion_matrix[(True, True)]\n",
    "tn = confusion_matrix[(False, False)]\n",
    "fp = confusion_matrix[(False, True)]\n",
    "fn = confusion_matrix[(True, False)]\n",
    "\n",
    "acc = accuracy(tp, fp, fn, tn)\n",
    "rec = recall(tp, fp, fn, tn)\n",
    "pre = precision(tp, fp, fn, tn)\n",
    "f1 = f1_score(tp, fp, fn, tn)\n",
    "\n",
    "print(\"Accuracy = \", acc)\n",
    "print(\"Recall = \", rec) \n",
    "print(\"Precision = \", pre)\n",
    "print(\"F1-score = \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spammiest_words ['zzzz', 'weight', 'hiring', 'your', 'warranties', 'rich', 'want', 'assistance', 'lowest', 'per']\n",
      "hammiest_words ['2', 'test', 'deployment', 'bliss', 'sept', 're', 'roman', 'bad', 'tough', 'name']\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_word(word: str, model: NaiveBayesClassifier) -> float:\n",
    "    \"\"\"uses bayes's theorem to compute p(spam | message contains word)\"\"\"\n",
    "    prob_if_spam, prob_if_ham = model._probabilities(word)\n",
    "        \n",
    "    return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "\n",
    "#sort by p(spam | message contains word) from smallest to largest \n",
    "words = sorted(classifier.words, key=lambda t: p_spam_given_word(t, classifier))\n",
    "\n",
    "#the highest predicted spam probabilities\n",
    "print(\"spammiest_words\", words[-10:])\n",
    "#the lowest predicted spam probabilities\n",
    "print(\"hammiest_words\", words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at the message content, not just the\tsubject\tline. Have to be careful how to deal with the message headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier takes into account every word that\tappears in the training\tset, even words that appear only once. Modify the classifier to accept an optional min_count threshhold and ignore tokens that don’t appear at least that many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tokenizer has no notion of similar words (e.g., “cheap” and “cheapest”). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. Creating a good stemmer function is\thard. People frequently use the\tPorter Stemmer. Example of a really simple stemmer function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 1 's' character at the end of the word\n",
    "def drop_final_s(word):\n",
    "    return re.sub(\"s$\", \"\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although our features are all of the form\t “message contains word $w_i$,” there’s no reason why this has to be the case. In the implementation, we could add extra features like “message contains a number” by creating phony tokens like *contains:number* and modifying the *tokenizer* to\temit them when appropriate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
